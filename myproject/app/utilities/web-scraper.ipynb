{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Scrape Product Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from seleniumwire import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: Set up proxy server\n",
    "username = \"brd-customer-hl_ab73b620-zone-datacenter10\"\n",
    "password = \"jz8qvx63tljn\"\n",
    "proxy_host = \"brd.superproxy.io\"\n",
    "proxy_port = \"22225\"\n",
    "\n",
    "proxy_options = { \n",
    "    'proxy': { \n",
    "        'http': f'http://{username}:{password}@{proxy_host}:{proxy_port}', \n",
    "        'https': f'https://{username}:{password}@{proxy_host}:{proxy_port}', \n",
    "        'no_proxy': 'localhost, 127.0.0.1'  # Bypass proxy for local addresses \n",
    "    } \n",
    "} \n",
    "\n",
    "# Step 1: Open website and close any window\n",
    "driver2 = webdriver.Chrome(seleniumwire_options=proxy_options)\n",
    "driver2.get('https://oliveyoung.com//')\n",
    "print(driver2.title)\n",
    "time.sleep(1)\n",
    "\n",
    " # Step 2: Close the 'Are you resident of CA?' window\n",
    "try:\n",
    "    resident_YN = WebDriverWait(driver2, 10).until(EC.presence_of_element_located((By.XPATH, \"/html/body/div[15]/div[2]/div/div[2]/div/button\")) )\n",
    "    time.sleep(2)\n",
    "    resident_YN.click()\n",
    "    print(\"Cleared 'Are you a CA resident' question\")\n",
    "except:\n",
    "    print(\"No resident CA question shown.\")\n",
    "\n",
    "\n",
    "# Step 3: Click the Catergories button -> choose Skincare\n",
    "categories_button = driver2.find_element(By.LINK_TEXT,\"Categories\")\n",
    "categories_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "skincare_button2 = driver2.find_element(By.LINK_TEXT,\"Face Masks\")\n",
    "skincare_button2.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 4: Get total page numbers\n",
    "more_page_button = driver2.find_element(By.CSS_SELECTOR,\".btn-page-more\")\n",
    "driver2.execute_script(\"arguments[0].scrollIntoView(true)\",more_page_button)\n",
    "pages_text = more_page_button.text\n",
    "page_start_index = pages_text.index(\"/\")+1\n",
    "page_end_index = len(pages_text)-1\n",
    "total_pages = int(pages_text[page_start_index:page_end_index])\n",
    "time.sleep(2)\n",
    "\n",
    "# Step 5: Scroll down to the 'More page' button and click it 'Total_pages' times\n",
    "for page in range(total_pages):\n",
    "    try:\n",
    "        more_page_button = driver2.find_element(By.CSS_SELECTOR,\".btn-page-more\")\n",
    "        more_page_button.click()\n",
    "        time.sleep(4)\n",
    "        WebDriverWait(driver2, 5).until(EC.presence_of_element_located((By.CLASS_NAME,\"unit-list\")) )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        print(\"Loading page did not complete.\")\n",
    "        break\n",
    "     \n",
    "print(f\"Loading page {total_pages} completed.\")\n",
    "\n",
    "# Step 6: Extract href links of products \n",
    "product_links = []\n",
    "product_count = 0\n",
    "all_links = driver2.find_elements(By.TAG_NAME,\"a\")\n",
    "\n",
    "for link in all_links:\n",
    "    all_hrefs = link.get_attribute(\"href\")\n",
    "    if all_hrefs and \"/product/\" in all_hrefs:\n",
    "        product_count += 1\n",
    "        product_links.append(all_hrefs)\n",
    "\n",
    "    # Step 6.1: There are 2 href links for each product, so I have to remove duplicates\n",
    "def distinct(product_links):\n",
    "    distinct_links = []\n",
    "    for l in product_links:\n",
    "        if l not in distinct_links:\n",
    "            distinct_links.append(l)\n",
    "    return distinct_links\n",
    "\n",
    "print(f\"All Product links found: \")   \n",
    "distinct_list = distinct(product_links)\n",
    "print(distinct_list)\n",
    "print(f\"Total link count: {len(distinct_list)}\") \n",
    "\n",
    "\n",
    "   # Step 6.2: Export product links to a .csv file\n",
    "df = pd.DataFrame(distinct_list, columns=['Link'])\n",
    "df.to_csv('ProductLinks-masks.csv')\n",
    "\n",
    "\n",
    "driver2.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Go to link and collect product data\n",
    "def get_product_data (driver3, prod_link):   \n",
    "    products = []\n",
    "    users = []\n",
    "    count = 0\n",
    "    for link in prod_link:\n",
    "        retries = 0\n",
    "        success = False\n",
    "        \n",
    "        while retries <= 3 and not success:\n",
    "            try:\n",
    "                prod_data = {'status': None,\n",
    "                        'brand': None,\n",
    "                        'prod_name': None,\n",
    "                        'price': None,\n",
    "                        'rating': None,\n",
    "                        'category':None,\n",
    "                        'subcategory': None,\n",
    "                        'ingredients': None,\n",
    "                        'reviews_no': None,\n",
    "                        'review': None,\n",
    "                        'link': None\n",
    "                        }\n",
    "\n",
    "                user_data = {'prod_name': None,\n",
    "                        'user_skin_type': None,\n",
    "                        'user_skin_concerns' : None\n",
    "                        }\n",
    "                \n",
    "                #driver3.set_window_size(1920,1080)    # #If compile in 1920 x 1080 res screen\n",
    "                driver3.get(link)\n",
    "                time.sleep(2)\n",
    "                prod_data['brand'] = get_brand_name(driver3)\n",
    "                prod_data['prod_name'] = get_product_name(driver3)\n",
    "                user_data['prod_name'] = get_product_name(driver3)\n",
    "                prod_data['price'] = get_price(driver3)\n",
    "                prod_data['rating'] = get_rating(driver3) \n",
    "                prod_data['reviews_no'] = get_review_no(driver3)\n",
    "                prod_data['category'], prod_data['subcategory'] = get_category(driver3)\n",
    "                prod_data['ingredients'] = get_ingredients(driver3)\n",
    "                prod_data['link'] = link\n",
    "\n",
    "                review_pages = get_review_pages(driver3)\n",
    "                if review_pages > 0:\n",
    "                    #translate_review(driver3)\n",
    "                    load_review_pages(driver3, review_pages)\n",
    "                    expand_review(driver3)\n",
    "                    prod_data['review'] = extract_review(driver3)\n",
    "                    user_data['user_skin_type'] = extract_user_skin_type(driver3)\n",
    "                    user_data['user_skin_concerns'] = extract_user_skin_concerns(driver3)\n",
    "\n",
    "                elif review_pages == 0 and prod_data['reviews_no'] > 0:\n",
    "                    #translate_review(driver3)\n",
    "                    expand_review(driver3)\n",
    "                    prod_data['review'] = extract_review(driver3)\n",
    "                    user_data['user_skin_type'] = extract_user_skin_type(driver3)\n",
    "                    user_data['user_skin_concerns'] = extract_user_skin_concerns(driver3)\n",
    "                    \n",
    "                else:\n",
    "                    prod_data['review'] = 0\n",
    "                    user_data['user_skin_type'] = 0\n",
    "                    user_data['user_skin_concerns'] = 0\n",
    "                    print(\"Zero review. No loading and translation required.\")\n",
    "                products.append(prod_data)\n",
    "                users.append(user_data)  \n",
    "                count += 1\n",
    "                success = True\n",
    "                print(f\"COMPLETED DATA EXTRACTION FROM {count} PRODUCT(S).\")\n",
    "\n",
    "            except TimeoutException:\n",
    "                retries += 1\n",
    "                if retries == 3:\n",
    "                    print(f\"Could not load link {count}th.\")\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                success = False\n",
    "                print(\"Page not found or no longer exists.\")\n",
    "                break\n",
    "    return products, users\n",
    "    \n",
    "\n",
    "    # Step 2.1: Extract brand and product name\n",
    "def get_brand_name(driver): \n",
    "    try:\n",
    "        brand_info = driver3.find_element(By.CLASS_NAME,\"prd-brand-info\")\n",
    "        brand_text = brand_info.text\n",
    "        brandname, product_name, sub_brand_text3 = brand_text.split('\\n', 2)\n",
    "        print(brandname)\n",
    "    except NoSuchElementException:\n",
    "            print(\"No brand info element found.\")\n",
    "            return None\n",
    "    except TimeoutException:\n",
    "            print(\"The product page never loaded.\")\n",
    "            return None\n",
    "    return brandname\n",
    "\n",
    "def get_product_name(driver): \n",
    "    try:\n",
    "        brand_info = driver3.find_element(By.CLASS_NAME,\"prd-brand-info\")\n",
    "        brand_text = brand_info.text\n",
    "        brandname, product_name, sub_brand_text3 = brand_text.split('\\n', 2)\n",
    "        print(product_name)\n",
    "    except NoSuchElementException:\n",
    "            print(\"No brand info element found.\")\n",
    "            return None\n",
    "    except TimeoutException:\n",
    "            print(\"The product page never loaded.\")\n",
    "            return None\n",
    "    return product_name\n",
    "\n",
    "    # Step 2.2: Extract price\n",
    "def get_price(driver):\n",
    "    price = driver3.find_element(By.CLASS_NAME,\"sale-price\")\n",
    "    price_text = price.text\n",
    "    price_start_index = price_text.index(\"$\")+1\n",
    "    price_end_index = len(price_text)-1\n",
    "    price_text = float(price_text[price_start_index:price_end_index])\n",
    "    print(price_text)\n",
    "    return price_text\n",
    "\n",
    "    # Step 2.3: Extract rating & review no\n",
    "def get_rating(driver):\n",
    "    review_info = driver3.find_element(By.CLASS_NAME,\"prd-rating-info\")\n",
    "    review_info_text = review_info.text\n",
    "    if '\\n' in review_info_text:\n",
    "        rating, r2 = review_info_text.split('\\n',1)\n",
    "\n",
    "        print(f\"Rating: \"+rating)\n",
    "        return rating\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \"\"\"\n",
    "def get_rating_review_no(driver):\n",
    "    review_info = driver3.find_element(By.CLASS_NAME,\"prd-rating-info\")\n",
    "    review_info_text = review_info.text\n",
    "    if '\\n' in review_info_text:\n",
    "        rating, r2 = review_info_text.split('\\n',1)\n",
    "        reviews_no = r2.split(\" \")[0]\n",
    "        if ',' in reviews_no:\n",
    "            trimmed_reviews_no = int(reviews_no.replace(',',''))\n",
    "        else: \n",
    "            trimmed_reviews_no = int(reviews_no)\n",
    "\n",
    "        print(f\"Rating: \"+rating)\n",
    "        print(f\"Reviews Count : {trimmed_reviews_no}\")\n",
    "        return rating, trimmed_reviews_no\n",
    "    else:\n",
    "        reviews_no = int(review_info_text.split(\" \")[0])\n",
    "        print(f\"Reviews Count : {reviews_no}\")\n",
    "        return None, reviews_no\n",
    "    \"\"\"\n",
    "    # Step 2.??: On 11/21 Olive Young added reviews from Korea, therefore, the review no extracted from \"prd-rating-info\" is no longer accurate\n",
    "def get_review_no(driver):\n",
    "    try:\n",
    "        review_number = driver3.find_element(By.CLASS_NAME,\"review-section-title\")\n",
    "        driver3.execute_script(\"arguments[0].scrollIntoView(true)\",review_number)\n",
    "        review_number_text = review_number.text\n",
    "        if \"Global Reviews\" in review_number_text and \"999+\" not in review_number_text:\n",
    "            review_number_text_start_index = review_number_text.index(\"(\")+1\n",
    "            review_number_text_end_index = len(review_number_text)-1\n",
    "            review_no = int(review_number_text[review_number_text_start_index:review_number_text_end_index])\n",
    "            print(f\"Review no: {review_no}\")\n",
    "            return review_no\n",
    "        elif \"Global Reviews\" in review_number_text & \"999+\" in review_number_text:\n",
    "            global_review_no = driver3.find_element(By.CLASS_NAME,\"review-list-count\").text\n",
    "            print(f\"Review no: {global_review_no}\")\n",
    "            return global_review_no\n",
    "        else:\n",
    "            return 0\n",
    "    except NoSuchElementException:\n",
    "        print(\"Cannot find global reviews\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    # Step 2.?: Get category of the product\n",
    "def get_category(driver):\n",
    "    try:\n",
    "        cat_list = driver3.find_elements(By.CLASS_NAME,\"loc_cat\")\n",
    "        for i, value in enumerate(cat_list):\n",
    "            if len(cat_list) > 2:\n",
    "                if i == 1:\n",
    "                    cat = value.text\n",
    "                elif i == 2:\n",
    "                    subcat = value.text\n",
    "            elif len(cat_list) <= 2:\n",
    "                if i == 1:\n",
    "                    cat = value.text\n",
    "                    subcat = None\n",
    "    except NoSuchElementException:\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Category: {cat}\")\n",
    "    print(f\"Sub-Category: {subcat}\")\n",
    "    return cat, subcat\n",
    "\n",
    "    # Step 2.4: Extract ingredients\n",
    "def get_ingredients(driver):\n",
    "    item_info_button = driver3.find_element(By.LINK_TEXT,\"Specific Item Info\")\n",
    "    item_info_button.click()\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        item_table = driver3.find_element(By.CLASS_NAME,\"prd-desc-tbl\")\n",
    "        ingredients = item_table.find_element(By.XPATH,'//tr[7]/td[1]').text\n",
    "        print(ingredients)\n",
    "        driver3.find_element(By.CSS_SELECTOR,\".modal-melg > div:nth-child(1) > div:nth-child(1) > button:nth-child(1)\").click()\n",
    "        return ingredients\n",
    "    except NoSuchElementException:\n",
    "        driver3.find_element(By.CSS_SELECTOR,\".modal-melg > div:nth-child(1) > div:nth-child(1) > button:nth-child(1)\").click()\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    #Step 2.5: Extract reviews \n",
    "\n",
    "        # Step 2.5.1: First, find the 'More' button element. Only click if it exists\n",
    "def get_review_pages(driver):\n",
    "    def find_more_button(element):\n",
    "        try:\n",
    "            return driver3.find_element(By.CLASS_NAME,element)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    more_page_button = find_more_button(element=\"review-list-more-btn\")\n",
    "    total_pages = 0\n",
    "    if more_page_button is not None:\n",
    "        driver3.execute_script(\"arguments[0].scrollIntoView(true)\",more_page_button)\n",
    "        pages_text = more_page_button.text\n",
    "        page_start_index = pages_text.index(\"/\")+1\n",
    "        page_end_index = len(pages_text)-1\n",
    "        total_pages = int(pages_text[page_start_index:page_end_index])\n",
    "        print(f\"Total number of review pages: {total_pages}\")\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        total_pages = 0\n",
    "        print(\"There are no review pages.\")\n",
    "    \n",
    "    return total_pages\n",
    "\n",
    "        #Step 2.5.2: Open up all review pages\n",
    "def load_review_pages(driver, n):\n",
    "    for i in range(n-1):\n",
    "        try:    \n",
    "            more_page_button = driver3.find_element(By.CLASS_NAME,\"review-list-more-btn\")\n",
    "            more_page_button.click()\n",
    "            time.sleep(5)\n",
    "            print(f\"Clicked {i+1} out of {n} buttons\")\n",
    "            #more_page_button.location_once_scrolled_into_view\n",
    "            #WebDriverWait(driver3,5).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\".btn-page-more\")))\n",
    "                   \n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {i+1} {e}\")\n",
    "            print(\"Loading page did not complete.\")\n",
    "            return\n",
    "\n",
    "        #Step 2.5.3: Translate all reviews (some are in Korean or Japanese)\n",
    "        # 11/13/24: This function is now obselete. Olive Young removed the translate toggle.\n",
    "def translate_review(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver3,5).until(EC.element_to_be_clickable((By.XPATH,\"/html/body/div[5]/div[1]/div[2]/div[3]/div[3]/div/div/div/div[2]/div[1]/div[2]/label\"))).click()\n",
    "    except NoSuchElementException:\n",
    "        print (\"No toggle found.\")\n",
    "\n",
    "        #Step 2.5.4: Expand all reviews \n",
    "def expand_review(driver):\n",
    "    expand_review_buttons = driver3.find_elements(By.CLASS_NAME,\"review-list-more-btn\")\n",
    "    print(f\"Found {len(expand_review_buttons)} review 'read-more' buttons.\")\n",
    "    for n, button in enumerate(expand_review_buttons):\n",
    "        try:\n",
    "            button.click()\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception : {e} Couldn't click the {n+1}th button.\")\n",
    "            return \n",
    "\n",
    "\n",
    "        #Step 2.5.4: Finally extract all review elements       \n",
    "def extract_review(driver):\n",
    "    review_list = []\n",
    "    review = driver3.find_elements(By.CLASS_NAME,\"review-unit-cont-comment\")\n",
    "    for n, value in enumerate(review):\n",
    "        review_text = value.text\n",
    "        review_list.append(review_text)\n",
    "    return review_list\n",
    "\n",
    "\n",
    "             # Extract user skin types  \n",
    "def extract_user_skin_type(driver):\n",
    "    skin_type = []\n",
    "    user_data_attr = driver3.find_elements(By.XPATH,'//dt[text()=\"Skin Type\"]/following-sibling::div/dd')\n",
    "    for attr in user_data_attr:\n",
    "        skin_type_attr = attr.text\n",
    "        if skin_type_attr != '':\n",
    "            skin_type.append(skin_type_attr)\n",
    "    print(f\"Skin Type found: {skin_type}\")\n",
    "    return skin_type\n",
    "\n",
    "    \n",
    "def extract_user_skin_concerns(driver):\n",
    "    skin_concerns = []\n",
    "            # Extract user skin concerns\n",
    "    user_data_attr = driver3.find_elements(By.XPATH,'//dt[text()=\"Skin Concern\"]/following-sibling::div/dd')\n",
    "    for attr in user_data_attr:\n",
    "        skin_type_attr = attr.text\n",
    "        if skin_type_attr != '':\n",
    "            skin_concerns.append(skin_type_attr)\n",
    "    print(f\"Skin Concerns found: {skin_concerns}\")\n",
    "    return skin_concerns\n",
    "\n",
    "\n",
    "#Step 0: Set up proxy server\n",
    "username = \"brd-customer-hl_ab73b620-zone-datacenter10\"\n",
    "password = \"jz8qvx63tljn\"\n",
    "proxy_host = \"brd.superproxy.io\"\n",
    "proxy_port = \"22225\"\n",
    "\n",
    "proxy_options = { \n",
    "    'proxy': { \n",
    "        'http': f'http://{username}:{password}@{proxy_host}:{proxy_port}', \n",
    "        'https': f'https://{username}:{password}@{proxy_host}:{proxy_port}', \n",
    "        'no_proxy': 'localhost, 127.0.0.1'  # Bypass proxy for local addresses \n",
    "    } \n",
    "}\n",
    "\n",
    "driver3 = webdriver.Chrome(seleniumwire_options=proxy_options)\n",
    "\n",
    "# Step 1: Read the csv file that contains all Olive Young products\n",
    "#df = pd.read_excel(r'C:\\Users\\buiva\\.vscode\\Web Scraping Test/ProductLinks-suncare-split.xlsx', sheet_name='Sheet_5')\n",
    "#prod_link = df['Link'].tolist()\n",
    "\n",
    "#  Test a specific Link: \n",
    "prod_link = [\"https://oliveyoung.com/product/detail?prdtNo=GA240322465\"]\n",
    "\n",
    "products, users = get_product_data(driver3, prod_link)\n",
    "#users = get_product_data(driver3, prod_link)\n",
    "\n",
    "#df = pd.DataFrame(products)\n",
    "#df.to_excel(\"ProductData-sunscreen-5.xlsx\", index = False)\n",
    "\n",
    "#df = pd.DataFrame(users)\n",
    "#df.to_excel(\"UserData-sunscreen-5.xlsx\", index = False)\n",
    "\n",
    "print(\"Excel files created.\")\n",
    "time.sleep(8)\n",
    "driver3.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_excel(r'C:\\Users\\buiva\\.vscode\\Web Scraping Test/ProductLinks-masks.xlsx', sheet_name='ProductLinks-masks')\n",
    "\n",
    "# Define the chunk size (number of rows per sheet)\n",
    "chunk_size = 100\n",
    "\n",
    "# Create a Pandas Excel writer object to write the data into an Excel file\n",
    "with pd.ExcelWriter(\"ProductLinks-masks-split.xlsx\", engine='xlsxwriter') as writer:\n",
    "    # Loop through the DataFrame in chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        # Select the chunk of data\n",
    "        chunk = df.iloc[i:i + chunk_size]\n",
    "        \n",
    "        # Write the chunk to a new sheet in the Excel file\n",
    "        sheet_name = f\"Sheet_{i // chunk_size + 1}\"  # Naming sheets as Sheet_1, Sheet_2, etc.\n",
    "        chunk.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"CSV file has been split into multiple sheets.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
